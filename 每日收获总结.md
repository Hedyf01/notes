2019.05.07
看了一点《信息简史》
blob:https://baike.baidu.com/34e2beeb-cc93-48af-8135-f7e23ceb2d90
囧知，很好的科普视频栏目

热力学第二定律-熵增，自然状态趋于无序混乱的本质：无序的概率>>有序

习以为常的事情，往往本后有这个世界最本质的规律。苹果落下来而不是飞上去（万有引力），墨滴入水中会混为一体（热力学第二定律，且符合大数定律）。我还能发现哪些呢？

python3的多进程优于p2，同样的进程数，python3顺利，2会出现部分进程运行一小段就不再运行，变成僵尸进程。深层原因不明。

hdfs不同集群可以都被一样的hdfs clinet访问么，只要知道地址。ip地址。或者域名地址？

pip install没网络，可以试着设置pip源，在.pip/.pipconf

刷了2题，Lc84,581

丢了钥匙串
做了小燕飞


1:25睡觉 简洁

--------
2019.05.08

花了近1h写周报
花了1h找钥匙串看监控，依旧没找到；室友帮忙重配了钥匙

发现一个现象：python多进程，子进程运行过程中一直等待，会被换pid。gpu机器的奇怪点在于同样的代码数据进程数，在另一台机器可以，在这台机器就不行。10个子进程有9个已经完成了却一直等待，没有被回收。

vim :%s/查找的字符串/替换的字符串/g 实现全局替换

花了1h+改小程序logo像素大小，发现了矢量图无底背景（灰）和白色背景的区别，无底背景与设备和软件环境设置有关，有时候是白，有时候是黑。

第一次用企业云盘。

了解到富牛途途可以炒股，平台。
港股美股需要外资银行账户和股票账户绑定，否则无法提现。不清楚交易所和证券公司的区别？

omgid不同设备不同软件不一样

买了保险

依旧没有运动

没有刷题

花了很长时间调参数跑数据，算了数据分布差异。没什么技术收获。

第一次用XX新闻畅听版本，体验了下

晚上花了一个半小时学习上海话，找到不少和家乡话的相似之处(j-g,这-个，药），以及不同点（h-w,c-z)等；兴趣是最好的动力，注意力是什么，你就是什么。

跟rn沟通依旧极其不爽
想走，但我必须先让自己强大。
有话语权！

睡觉，0:47

------------
5月16日0:25，最近的睡觉时间都在一点以后，甚至两点。
这几天醉心于工作。
昨天和同事有交流game方面的工作，有很多收获。所以开放才是最重要的提升手段。
要开放，也要自省、冷静、总结。
很多天没总结了。
工作上事务性的推动不说。想想实际的收获。

健康是负收获，熬夜太多。现在耳朵又耳鸣+gz湿疹

技术上：
熟悉linux命令：
磁盘使用情况：ds -h
目录文件夹大小： 
总和 du -sh
各个 du -h --max-depth=1

shell的编程：
第一次变量命名不用声明，后续引用要加$
for day in ${day[*]}
遍历数组要加*或者@，否则只能取数组的第一个元素

hadoop有个ugi的配置参数，是加用户的

pyspark，dataframe
df.groupBy(XX).agg(funcs.count('col').alias('name'))
df=df.drop('col')
df=df.union(df2)

scp带端口传输
scp -P prot file user@host:~/destination

tar
压缩 tar -czvf xx.tar.gz 要压缩的文件或者目录

crontab -e编辑
不要忘记路径用绝对路径

nohup python xx.py args > xx.log 2>&1 &

killall python 

### word2vec的调参数经验：
wordsize大，2000万数据，200维
window小，2比3好
minc小但也不能太小，4比3好，3比5好

机器学习的一些本质：
ctr预估是0或者1的预测，每次预测独立，是伯努力分布

*学习到的向量要和预测的东西在一个向量空间，否则这个向量没有意义*。
比如word2vec训练的video和以此行为序列得到的用户是在一个向量空间的，因此可以算相似度。
但学到的word直接加到end to end的ctr模型中，因为二者不在一个向量空间，在模型更新过程中，word重新被更新，效果不一定好。
*模型会收敛，随机初始化和训练好的word去初始化最终效果是一样的，只是后者可能会加快收敛速度*

> 疑问：只要模型收敛，效果都一样么？


推荐的流程：

召回-rank（ctr是重要或者唯一指标）-rerank（排版，多样性等）
召回是多路召回的。

> 有一个快速检索相似icf的方法，覆盖可以达到80%

双塔模型
user一边，item一边
训练好item事先，可以加快。


陆奇的创业营

陆奇的AI创业的分享知乎


1:03 还是会被朋友圈吸引，当下的我最重要的是养成耐力和中断力，管理好自己的注意力，可以坚持个一周！

---------
2019.05.16 23:10 在公司

word2vec有两种方法，cbow(周围词预测中心词），skip-gram（中心词预测周围词）
gensim默认的word2vec是cbow，sg参数=1，就是skip-gram

cbow训练速度比skip-gram快
skip-gram更精细，对于低频词效果更好。也更适合小数据

airbnb的房屋推荐也用了word2vec来把房屋embedding

> 粗排和精排的区别

在行为序列建模上，引入时间衰减效果很好，从0.9到0.4效果不断变好，0.5到0.4有显著提升，不明显。用户越时间越近的行为越有用。

早上洗漱又听了luqi的AI创业知乎课

农业时代的本质是光合作用
AI一定要是活数据
AI的本质是学习知识


*做logo时要简单，要突出中心，要整个图片元素是一个整体，四个位置+干杯就分散了*

某些环境下，python print中文字符会有ASCII编码错误，通过设置系统编码解决。export LANG=en_US.UTF-8

> 要学习word2vec的本质

想想自己的核心竞争力，不要真的沦为码农！每天学习的太少，太多的体力劳动而已。
时间有限！身体更可贵！

------------
5月23日21:51 在公司
5月20日那天生病了，感冒严重，可能有发烧，很晕，睡不醒的感觉，那天休息的比较早，结束了一个多月来1点以后甚至2点3点以后的熬夜。
最近在搞index的事情，主要是对c++和shell更熟悉了一些，其他收获不大。
要赶紧记下来

*二进制读取会比文本快很多*
因为文本比如‘1’需要对应字符串数据类型再解码对应到内存，而二进制的1直接就可以以int的方式存储。可以直接映射到内存的某一块

